{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spanish language records deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objectives\n",
    "- Read in data\n",
    "- Apply cleaning rules\n",
    "- Apply phonetics encoding\n",
    "- Construct identifiers\n",
    "- Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 1.2.1\n",
      "Numpy version: 1.19.4\n",
      "Joblib version: 0.17.0\n",
      "Seaborn version: 0.10.0\n",
      "Plotly version: 3.9.0\n",
      "PyArrow version: 3.0.0\n"
     ]
    }
   ],
   "source": [
    "# standard libraries\n",
    "import time\n",
    "import re # package to perform regular expressions\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pyarrow # Fast reading of parquets\n",
    "\n",
    "# letters of the alphabet libraries\n",
    "import string\n",
    "from string import ascii_lowercase, ascii_uppercase \n",
    "from random import choice\n",
    "from functools import reduce\n",
    "import xml.etree.ElementTree as et #xml\n",
    "\n",
    "#phonetics\n",
    "import phonetics\n",
    "\n",
    "# vizualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "\n",
    "# date formatter\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "\n",
    "# modeling libraries\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Import joblib for data persistance\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# check versions\n",
    "print(f'Pandas version: {pd.__version__}')\n",
    "print(f'Numpy version: {np.__version__}')\n",
    "print(f'Joblib version: {joblib.__version__}')\n",
    "print(f'Seaborn version: {sns.__version__}')\n",
    "print(f'Plotly version: {plotly.__version__}')\n",
    "print(f'PyArrow version: {pyarrow.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def catvardistribution(data,var,x,y,rot):\n",
    "    print(data[var].value_counts(dropna=False))\n",
    "    fig, ax = plt.subplots(figsize=(x,y)) # Set figure size\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    ax = sns.countplot(x=data[var], data=data, palette='Reds')\n",
    "    plt.xticks(rotation=rot)\n",
    "    plt.show()\n",
    "\n",
    "def boxplot(data,var):\n",
    "    print(data[var].describe())\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    ax = sns.boxplot(x=data[var])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "date = datetime.datetime.now().date()\n",
    "directory_input = \"C:/...\" #set input directory\n",
    "directory_output = \"C:/...\" #set output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read parquet to pandas dataframe\n",
    "d = pd.read_parquet(f'{directory_input}/data.parquet.gzip')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preserve original dataframe copy\n",
    "d_original = d.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_names(df, column):\n",
    "    df[column] = df[column].str.replace(r'[\\s]+','') # remove spaces\n",
    "    df[column] = df[column].str.replace(r'[\\d]+','') # remove numerical digits\n",
    "    df[column] = df[column].str.replace('Á','A') # replace A with accent mark\n",
    "    df[column] = df[column].str.replace('É','E') # replace E with accent mark\n",
    "    df[column] = df[column].str.replace('Í','I') # replace I with accent mark\n",
    "    df[column] = df[column].str.replace('Ó','O') # replace O with accent mark\n",
    "    df[column] = df[column].str.replace('Ú','U') # replace U with accent mark\n",
    "    df[column] = df[column].str.replace('Ì','I') # replace I with reverse accent mark\n",
    "    df[column] = df[column].str.replace('Ñ','N') # replace Ñ with N\n",
    "    df[column] = df[column].str.replace('[^A-Za-z]+', '') # remove non-Latin letters\n",
    "    df[column] = df[column].str.lower() #lower case names\n",
    "    df[column] = np.where(df[column].isnull(),'xxx',df[column]) # replace nulls with `xxx` \n",
    "    df[column] = np.where(df[column]==\"\",'xxx',df[column]) # replace nulls with `xxx`\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export excel file with missing or placeholder `GENDER`, `LAST_NAME` or `FIRST_NAME`. These records cannot be deduplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "noidentifier = d[(d['GENDER']=='Not Defined') | (d['FIRST_NAME'].str[0:6]=='XXXXXX') | (d['LAST_NAME'].str[0:6]=='XXXXXX')]\n",
    "print(len(noidentifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "noidentifier.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Percentage of data set that are dummies {len(noidentifier)/len(d_original)*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop superfluous columns\n",
    "noidentifier = noidentifier.drop(list(noidentifier.columns[75:99]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dummy records to parquet\n",
    "start = time.time()\n",
    "print('Writing pandas dataframe to parquet format..')\n",
    "noidentifier.to_parquet(f'{directory_output}/noidentifier.parquet.gzip', compression='gzip')\n",
    "print(f\"... completed job in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop these not-identifiable records from the data for deduplication\n",
    "d = d.drop(noidentifier.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "d = d.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "catvardistribution(d,'GENDER',x=5,y=5,rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that there are zero records with Not Defined Gender\n",
    "len(d[d.GENDER=='Not Defined']) ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect concentration of birthdates\n",
    "d['birthmonthday'].value_counts(dropna=False)/len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create back up of original whole name columns\n",
    "for col in d[['FIRST_NAME','MIDDLE_NAME','LAST_NAME']]:\n",
    "    d[f\"{col}_backup\"] = d[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of names variables\n",
    "\n",
    "first_names = list()\n",
    "for col in d.columns:\n",
    "    if col.startswith('FIRST_NAME_'):\n",
    "        first_names.append(col)\n",
    "\n",
    "middle_names = list()\n",
    "for col in d.columns:\n",
    "    if col.startswith('MIDDLE_NAME_'):\n",
    "        middle_names.append(col)\n",
    "\n",
    "last_names = list()\n",
    "for col in d.columns:\n",
    "    if col.startswith('LAST_NAME_'):\n",
    "        last_names.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [first_names + middle_names + last_names]\n",
    "names_decomposed = names[0].copy()\n",
    "print(names_decomposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for clean names decomposition, remove name backups\n",
    "names_backup = ['FIRST_NAME_backup','MIDDLE_NAME_backup','LAST_NAME_backup']\n",
    "for x in names_backup:\n",
    "    names_decomposed.remove(x)\n",
    "print(names_decomposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply data cleaning rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean names\n",
    "for name in d[names[0]]:\n",
    "    d = clean_names(d, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names[0]:\n",
    "    print(name)\n",
    "    print(d[name].value_counts(dropna=False).head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate columns for each of the first 3 letters of the each of the names\n",
    "\n",
    "# generate lists for each letter variable\n",
    "letter1 = d[names[0]].columns.astype(str) + '_letter1'\n",
    "letter2 = d[names[0]].columns.astype(str) + '_letter2'\n",
    "letter3 = d[names[0]].columns.astype(str) + '_letter3'\n",
    "\n",
    "letter1 = letter1.tolist()\n",
    "letter2 = letter2.tolist()\n",
    "letter3 = letter3.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variable for first letter\n",
    "for letter,name in zip(letter1, names[0]):\n",
    "    d[letter] = d[name].str[0]\n",
    "\n",
    "# create variable for second letter\n",
    "for letter,name in zip(letter2, names[0]):\n",
    "    d[letter] = d[name].str[1]\n",
    "\n",
    "# create variable for third letter\n",
    "for letter,name in zip(letter3, names[0]):\n",
    "    d[letter] = d[name].str[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phonetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate lists for each phonetics encoding type\n",
    "soundex = d[names[0]].columns.astype(str) + '_soundex'\n",
    "metaphone = d[names[0]].columns.astype(str) + '_metaphone'\n",
    "dmetaphone = d[names[0]].columns.astype(str) + '_dmetaphone'\n",
    "\n",
    "soundex = soundex.tolist()\n",
    "metaphone = metaphone.tolist()\n",
    "dmetaphone = dmetaphone.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create soundex features\n",
    "for phone,name in zip(soundex, names[0]):\n",
    "    d[phone] = d[name].apply(lambda x: phonetics.soundex(x))      \n",
    "        \n",
    "# create metaphone features\n",
    "for phone,name in zip(metaphone, names[0]):\n",
    "    d[phone] = d[name].apply(lambda x: phonetics.metaphone(x)) \n",
    "\n",
    "# create double metaphone features\n",
    "for phone,name in zip(dmetaphone, names[0]):\n",
    "    d[phone] = d[name].apply(lambda x: phonetics.dmetaphone(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check results\n",
    "for name in soundex:\n",
    "    print(name)\n",
    "    print(d[name].value_counts(dropna=False).head())\n",
    "    \n",
    "for name in metaphone:\n",
    "    print(name)\n",
    "    print(d[name].value_counts(dropna=False).head())\n",
    "    \n",
    "for name in dmetaphone:\n",
    "    print(name)\n",
    "    print(d[name].value_counts(dropna=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for partial names decomposition, remove name backup letters\n",
    "letter1_backup = ['FIRST_NAME_backup_letter1','MIDDLE_NAME_backup_letter1','LAST_NAME_backup_letter1']\n",
    "letter2_backup = ['FIRST_NAME_backup_letter2','MIDDLE_NAME_backup_letter2','LAST_NAME_backup_letter2']\n",
    "letter3_backup = ['FIRST_NAME_backup_letter3','MIDDLE_NAME_backup_letter3','LAST_NAME_backup_letter3']\n",
    "\n",
    "for x in letter1_backup:\n",
    "    letter1.remove(x)\n",
    "\n",
    "for x in letter2_backup:\n",
    "    letter2.remove(x)\n",
    "\n",
    "for x in letter3_backup:\n",
    "    letter3.remove(x)\n",
    "    \n",
    "print(letter1)\n",
    "print(letter2)\n",
    "print(letter3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for phonetics decomposition, remove name backup letters\n",
    "soundex_backup = ['FIRST_NAME_backup_soundex','MIDDLE_NAME_backup_soundex','LAST_NAME_backup_soundex']\n",
    "metaphone_backup = ['FIRST_NAME_backup_metaphone','MIDDLE_NAME_backup_metaphone','LAST_NAME_backup_metaphone']\n",
    "dmetaphone_backup = ['FIRST_NAME_backup_dmetaphone','MIDDLE_NAME_backup_dmetaphone','LAST_NAME_backup_dmetaphone']\n",
    "\n",
    "for x in soundex_backup:\n",
    "    soundex.remove(x)\n",
    "\n",
    "for x in metaphone_backup:\n",
    "    metaphone.remove(x)\n",
    "\n",
    "for x in dmetaphone_backup:\n",
    "    dmetaphone.remove(x)\n",
    "    \n",
    "print(soundex)\n",
    "print(metaphone)\n",
    "print(dmetaphone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of engineered features\n",
    "feature_dict = {'names':names[0], 'letter1': letter1, 'letter2': letter2, 'letter3': letter3, \n",
    "                'soundex': soundex, 'metaphone': metaphone, 'dmetaphone': dmetaphone, 'names_decomposed': names_decomposed}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct identifiers for deduplication (letter-based, whole name, soundex, and metaphone-based identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifier 1 (whole names - raw without cleaning + BIRTHDATE)\n",
    "d['raw_wholename_id'] = d['FIRST_NAME'] + d['LAST_NAME'] + d['GENDER'] + d['birthdate'] \n",
    "\n",
    "# identifier 2 (whole names - with cleaning + BIRTHDATE)\n",
    "d['clean_wholename_id'] = d['FIRST_NAME_backup'] + d['MIDDLE_NAME_backup']+ d['LAST_NAME_backup'] \\\n",
    "                        + d['GENDER'] + d['birthdate']\n",
    "\n",
    "# identifier 2b (whole names - with cleaning + MOBILE_NUMBER)\n",
    "d['clean_wholename_mobile_id'] = d['FIRST_NAME_backup'] + d['MIDDLE_NAME_backup']+ d['LAST_NAME_backup'] \\\n",
    "                        + d['GENDER'] + d['MOBILE_NUMBER']\n",
    "\n",
    "# identifier 2c (whole names - with cleaning + BIRTHYEAR + BIRTHMONTH)\n",
    "d['clean_wholename_birthyearmonth_id'] = d['FIRST_NAME_backup'] + d['MIDDLE_NAME_backup']+ d['LAST_NAME_backup'] \\\n",
    "                        + d['GENDER'] + d['birthyear_str'] + d['birthmonth_str']\n",
    "\n",
    "# identifier 2d (whole names - with cleaning + BIRTHYEAR + BIRTHDAY)\n",
    "d['clean_wholename_birthyearday_id'] = d['FIRST_NAME_backup'] + d['MIDDLE_NAME_backup']+ d['LAST_NAME_backup'] \\\n",
    "                        + d['GENDER'] + d['birthyear_str'] + d['birthday_str']\n",
    "\n",
    "# identifier 2e (whole names - with cleaning + BIRTHMONTH + BIRTHDAY)\n",
    "d['clean_wholename_birthmonthday_id'] = d['FIRST_NAME_backup'] + d['MIDDLE_NAME_backup']+ d['LAST_NAME_backup'] \\\n",
    "                        + d['GENDER'] + d['birthmonth_str'] + d['birthday_str']\n",
    "\n",
    "# identifier 3 (whole decomposed names - with cleaning)\n",
    "d['clean_wholename_decomposed_id'] = d[names_decomposed].sum(axis=1).astype(str) + d['GENDER'] + d['birthdate'] \n",
    "\n",
    "# identifier 4 (first 3 letters of names - \"Paso 5 identifier\" + BIRTHDATE)\n",
    "d['partialnames_id'] = d['FIRST_NAME_backup_letter1'] + d['FIRST_NAME_backup_letter2'] + d['FIRST_NAME_backup_letter3'] \\\n",
    "                     + d['LAST_NAME_backup_letter1'] + d['LAST_NAME_backup_letter2'] + d['LAST_NAME_backup_letter3'] \\\n",
    "                     + d['GENDER'] + d['birthdate']\n",
    "\n",
    "# identifier 4b (first 3 letters of names - \"Paso 5 identifier\" + MOBILE_NUMBER)\n",
    "d['partialnames_mobile_id'] = d['FIRST_NAME_backup_letter1'] + d['FIRST_NAME_backup_letter2'] + d['FIRST_NAME_backup_letter3'] \\\n",
    "                     + d['LAST_NAME_backup_letter1'] + d['LAST_NAME_backup_letter2'] + d['LAST_NAME_backup_letter3'] \\\n",
    "                     + d['GENDER'] + d['MOBILE_NUMBER']\n",
    "\n",
    "# identifier 4c (first 3 letters of names - \"Paso 5 identifier\" + BIRTHYEAR + BIRTHMONTH)\n",
    "d['partialnames_birthyearmonth_id'] = d['FIRST_NAME_backup_letter1'] + d['FIRST_NAME_backup_letter2'] + d['FIRST_NAME_backup_letter3'] \\\n",
    "                     + d['LAST_NAME_backup_letter1'] + d['LAST_NAME_backup_letter2'] + d['LAST_NAME_backup_letter3'] \\\n",
    "                     + d['GENDER'] + d['birthyear_str'] + d['birthmonth_str']\n",
    "\n",
    "# identifier 4d (first 3 letters of names - \"Paso 5 identifier\" + BIRTHYEAR + BIRTHDAY)\n",
    "d['partialnames_birthyearday_id'] = d['FIRST_NAME_backup_letter1'] + d['FIRST_NAME_backup_letter2'] + d['FIRST_NAME_backup_letter3'] \\\n",
    "                     + d['LAST_NAME_backup_letter1'] + d['LAST_NAME_backup_letter2'] + d['LAST_NAME_backup_letter3'] \\\n",
    "                     + d['GENDER'] + d['birthyear_str'] + d['birthday_str']\n",
    "\n",
    "# identifier 4e (first 3 letters of names - \"Paso 5 identifier\" + BIRTHMONTH + BIRTHDAY)\n",
    "d['partialnames_birthmonthday_id'] = d['FIRST_NAME_backup_letter1'] + d['FIRST_NAME_backup_letter2'] + d['FIRST_NAME_backup_letter3'] \\\n",
    "                     + d['LAST_NAME_backup_letter1'] + d['LAST_NAME_backup_letter2'] + d['LAST_NAME_backup_letter3'] \\\n",
    "                     + d['GENDER'] + d['birthmonth_str'] + d['birthday_str']\n",
    "\n",
    "\n",
    "# identifier 5 (first 3 letters of separate names)\n",
    "d['partialnames_decomposed_id'] = d[letter1].sum(axis=1).astype(str) + d[letter2].sum(axis=1).astype(str) + d[letter3].sum(axis=1).astype(str) + d['GENDER'] + d['birthdate']\n",
    "\n",
    "# identifier 6 (Soundex - Non-decomposed names + BIRTHDATE)\n",
    "d['soundex_id'] = d['FIRST_NAME_backup_soundex']+ d['MIDDLE_NAME_backup_soundex'] + d['LAST_NAME_backup_soundex'] \\\n",
    "                + d['GENDER'] + d['birthdate']\n",
    "\n",
    "# identifier 6b (Soundex - Non-decomposed names + MOBILE_NUMBER)\n",
    "d['soundex_mobile_id'] = d['FIRST_NAME_backup_soundex']+ d['MIDDLE_NAME_backup_soundex'] + d['LAST_NAME_backup_soundex'] \\\n",
    "                + d['GENDER'] + d['MOBILE_NUMBER']\n",
    "\n",
    "# identifier 6c (Soundex - Non-decomposed names + BIRTHYEAR + BIRTHMONTH)\n",
    "d['soundex_birthyearmonth_id'] = d['FIRST_NAME_backup_soundex']+ d['MIDDLE_NAME_backup_soundex'] + d['LAST_NAME_backup_soundex'] \\\n",
    "                + d['GENDER'] + d['birthyear_str'] + d['birthmonth_str']\n",
    "\n",
    "# identifier 6d (Soundex - Non-decomposed names + BIRTHYEAR + BIRTHDAY)\n",
    "d['soundex_birthyearday_id'] = d['FIRST_NAME_backup_soundex']+ d['MIDDLE_NAME_backup_soundex'] + d['LAST_NAME_backup_soundex'] \\\n",
    "                + d['GENDER'] + d['birthyear_str'] + d['birthday_str']\n",
    "\n",
    "# identifier 6e (Soundex - Non-decomposed names + BIRTHMONTH + BIRTHDAY)\n",
    "d['soundex_birthmonthday_id'] = d['FIRST_NAME_backup_soundex']+ d['MIDDLE_NAME_backup_soundex'] + d['LAST_NAME_backup_soundex'] \\\n",
    "                + d['GENDER'] + d['birthmonth_str'] + d['birthday_str']\n",
    "\n",
    "\n",
    "# identifier 7 (Soundex - Decomposed names)\n",
    "d['soundex_decomposed_id'] = d[soundex].sum(axis=1).astype(str) + d['GENDER'] + d['birthdate']\n",
    "\n",
    "# identifier 8 (Metaphone - Non-decomposed names + BIRTHDATE)\n",
    "d['metaphone_id'] = d['FIRST_NAME_backup_metaphone'] + d['MIDDLE_NAME_backup_metaphone'] + d['LAST_NAME_backup_metaphone'] \\\n",
    "                  + d['GENDER'] + d['birthdate']        \n",
    "\n",
    "# identifier 8b (Metaphone - Non-decomposed names + MOBILE_NUMBER)\n",
    "d['metaphone_mobile_id'] = d['FIRST_NAME_backup_metaphone'] + d['MIDDLE_NAME_backup_metaphone'] + d['LAST_NAME_backup_metaphone'] \\\n",
    "                  + d['GENDER'] + d['MOBILE_NUMBER']        \n",
    "\n",
    "# identifier 8c (Metaphone - Non-decomposed names + BIRTHYEAR + BIRTHMONTH)\n",
    "d['metaphone_birthyearmonth_id'] = d['FIRST_NAME_backup_metaphone'] + d['MIDDLE_NAME_backup_metaphone'] + d['LAST_NAME_backup_metaphone'] \\\n",
    "                  + d['GENDER'] + d['birthyear_str'] + d['birthmonth_str']     \n",
    "\n",
    "# identifier 8d (Metaphone - Non-decomposed names + BIRTHYEAR + BIRTHDAY)\n",
    "d['metaphone_birthyearday_id'] = d['FIRST_NAME_backup_metaphone'] + d['MIDDLE_NAME_backup_metaphone'] + d['LAST_NAME_backup_metaphone'] \\\n",
    "                  + d['GENDER'] + d['birthyear_str'] + d['birthday_str']     \n",
    "\n",
    "# identifier 8e (Metaphone - Non-decomposed names + BIRTHMONTH + BIRTHDAY)\n",
    "d['metaphone_birthmonthday_id'] = d['FIRST_NAME_backup_metaphone'] + d['MIDDLE_NAME_backup_metaphone'] + d['LAST_NAME_backup_metaphone'] \\\n",
    "                  + d['GENDER'] + d['birthmonth_str'] + d['birthday_str']     \n",
    "\n",
    "\n",
    "# identifier 9 (Metaphone - Decomposed names)\n",
    "d['metaphone_decomposed_id'] = d[metaphone].sum(axis=1).astype(str) + d['GENDER'] + d['birthdate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of generated identifiers\n",
    "id_list = ['raw_wholename_id',\n",
    "           'clean_wholename_id','clean_wholename_mobile_id', 'clean_wholename_birthyearmonth_id', 'clean_wholename_birthyearday_id',\n",
    "           'clean_wholename_birthmonthday_id','clean_wholename_decomposed_id',\n",
    "           'partialnames_id','partialnames_mobile_id','partialnames_birthyearmonth_id','partialnames_birthyearday_id',\n",
    "           'partialnames_birthmonthday_id','partialnames_decomposed_id',\n",
    "           'soundex_id','soundex_mobile_id','soundex_birthyearmonth_id','soundex_birthyearday_id','soundex_birthmonthday_id',\n",
    "           'soundex_decomposed_id',\n",
    "           'metaphone_id','metaphone_mobile_id','metaphone_birthyearmonth_id','metaphone_birthyearday_id','metaphone_birthmonthday_id',\n",
    "           'metaphone_decomposed_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicationanalyzer(data,id_list):\n",
    "    ''' Assess the number and percentage of duplicates for each variable'''\n",
    "    for var in id_list:\n",
    "        # print the number of duplicates\n",
    "        duplicates=data[data[var].duplicated(keep=False)]\n",
    "        print(var)\n",
    "        print('Number of duplicates: ',len(duplicates))\n",
    "\n",
    "        # print the percentage of duplicates\n",
    "        percentage = len(duplicates)/len(data)\n",
    "        print('Percentage of duplicates: ',percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicationanalyzer(d,id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataframes with duplicates\n",
    "dup1 = d[d['raw_wholename_id'].duplicated(keep=False)].sort_values(by=['raw_wholename_id'])\n",
    "dup2 = d[d['clean_wholename_id'].duplicated(keep=False)].sort_values(by=['clean_wholename_id'])\n",
    "dup3 = d[d['clean_wholename_mobile_id'].duplicated(keep=False)].sort_values(by=['clean_wholename_mobile_id'])\n",
    "dup4 = d[d['clean_wholename_birthyearmonth_id'].duplicated(keep=False)].sort_values(by=['clean_wholename_birthyearmonth_id'])\n",
    "dup5 = d[d['clean_wholename_birthyearday_id'].duplicated(keep=False)].sort_values(by=['clean_wholename_birthyearday_id'])\n",
    "dup6 = d[d['clean_wholename_birthmonthday_id'].duplicated(keep=False)].sort_values(by=['clean_wholename_birthmonthday_id'])\n",
    "dup7 = d[d['clean_wholename_decomposed_id'].duplicated(keep=False)].sort_values(by=['clean_wholename_decomposed_id'])\n",
    "\n",
    "dup8 = d[d['partialnames_id'].duplicated(keep=False)].sort_values(by=['partialnames_id'])\n",
    "dup9 = d[d['partialnames_mobile_id'].duplicated(keep=False)].sort_values(by=['partialnames_mobile_id'])\n",
    "dup10 = d[d['partialnames_birthyearmonth_id'].duplicated(keep=False)].sort_values(by=['partialnames_birthyearmonth_id'])\n",
    "dup11 = d[d['partialnames_birthyearday_id'].duplicated(keep=False)].sort_values(by=['partialnames_birthyearday_id'])\n",
    "dup12 = d[d['partialnames_birthmonthday_id'].duplicated(keep=False)].sort_values(by=['partialnames_birthmonthday_id'])\n",
    "dup13 = d[d['partialnames_decomposed_id'].duplicated(keep=False)].sort_values(by=['partialnames_decomposed_id'])\n",
    "\n",
    "dup14 = d[d['soundex_id'].duplicated(keep=False)].sort_values(by=['soundex_id'])\n",
    "dup15 = d[d['soundex_mobile_id'].duplicated(keep=False)].sort_values(by=['soundex_mobile_id'])\n",
    "dup16 = d[d['soundex_birthyearmonth_id'].duplicated(keep=False)].sort_values(by=['soundex_birthyearmonth_id'])\n",
    "dup17 = d[d['soundex_birthyearday_id'].duplicated(keep=False)].sort_values(by=['soundex_birthyearday_id'])\n",
    "dup18 = d[d['soundex_birthmonthday_id'].duplicated(keep=False)].sort_values(by=['soundex_birthmonthday_id'])\n",
    "dup19 = d[d['soundex_decomposed_id'].duplicated(keep=False)].sort_values(by=['soundex_decomposed_id'])\n",
    "\n",
    "dup20 = d[d['metaphone_id'].duplicated(keep=False)].sort_values(by=['metaphone_id'])\n",
    "dup21 = d[d['metaphone_mobile_id'].duplicated(keep=False)].sort_values(by=['metaphone_mobile_id'])\n",
    "dup22 = d[d['metaphone_birthyearmonth_id'].duplicated(keep=False)].sort_values(by=['metaphone_birthyearmonth_id'])\n",
    "dup23 = d[d['metaphone_birthyearday_id'].duplicated(keep=False)].sort_values(by=['metaphone_birthyearday_id'])\n",
    "dup24 = d[d['metaphone_birthmonthday_id'].duplicated(keep=False)].sort_values(by=['metaphone_birthmonthday_id'])\n",
    "dup25 = d[d['metaphone_decomposed_id'].duplicated(keep=False)].sort_values(by=['metaphone_decomposed_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of duplicates\n",
    "dup_list = [dup1, dup2, dup3, dup4, dup5, dup6, dup7, dup8, dup9, dup10, dup11, dup12, dup13, dup14, dup15, dup16, dup17, dup18,\n",
    "           dup19, dup20, dup21, dup22, dup23, dup24, dup25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups =[] # list of total duplicates in each duplicates set\n",
    "dup_pct =[] # list of percentage of duplicates from each duplicates set\n",
    "for i in range(0,len(dup_list)):\n",
    "    dups.append(len(dup_list[i]))\n",
    "    dup_pct.append(len(dup_list[i])/len(d))\n",
    "print(dups)\n",
    "print(dup_pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine `STATUS` of duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dup in dup_list:\n",
    "    print(dup['STATUS'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup1[['FIRST_NAME','MIDDLE_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','REGION','MOBILE_NUMBER','raw_wholename_id']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup2[['FIRST_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','REGION','MOBILE_NUMBER','clean_wholename_id']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup3[['FIRST_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','REGION','MOBILE_NUMBER','clean_wholename_mobile_id']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup4[['FIRST_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','REGION','MOBILE_NUMBER','clean_wholename_birthyearmonth_id']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup5[['FIRST_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','REGION','MOBILE_NUMBER','clean_wholename_birthyearday_id']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dup6[['FIRST_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','REGION','MOBILE_NUMBER','clean_wholename_birthmonthday_id']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup7[['FIRST_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','REGION','MOBILE_NUMBER','clean_wholename_decomposed_id']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup14[['FIRST_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','REGION','MOBILE_NUMBER','soundex_id']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup20[['FIRST_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','REGION','MOBILE_NUMBER','metaphone_id']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot proportion of duplicates\n",
    "x = ['Raw Whole Names','Clean Whole Names','Clean Whole Names - Mobile',\n",
    "     'Clean Whole Names - Year + Month','Clean Whole Names - Year + Day','Clean Whole Names - Month + Day', 'Clean Whole Names - Decomposed',\n",
    "     'Partial Names (Paso 5)','Partial Names - Mobile','Partial Names - Year + Month', 'Partial Names - Year + Day', 'Partial Names - Month + Day',\n",
    "     'Partial Names - Decomposed',\n",
    "     'Soundex','Soundex - Mobile','Soundex - Year + Month','Soundex - Year + Day','Soundex - Month + Day',\n",
    "     'Soundex-Decomposed',\n",
    "     'Metaphone','Metaphone - Mobile','Metaphone - Year + Month','Metaphone - Year + Day','Metaphone - Month + Day',\n",
    "     'Metaphone-Decomposed']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,7)) # Set figure size\n",
    "ax = sns.barplot(x = x, y = dup_pct, palette = 'Blues', edgecolor = 'b')\n",
    "ax.set_title(\"Duplicates as a percentage of total by Identifier\", fontsize=18)\n",
    "ax.set_xlabel(\"Identifier\", fontsize=18)\n",
    "ax.set_xticklabels(x, rotation=90, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot total number of duplicates\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,7)) # Set figure size\n",
    "ax = sns.barplot(x = x, y = dups, palette = 'Purples', edgecolor = 'b')\n",
    "ax.set_title(\"Duplicates by Identifier\", fontsize=18)\n",
    "ax.set_xlabel(\"Identifier\", fontsize=18)\n",
    "ax.set_xticklabels(x, rotation=90, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot comparing duplicates considered Active\n",
    "active_dups =[]\n",
    "for dup in dup_list:\n",
    "    active_dups.append(len(dup[dup['STATUS']=='ACTIVE']))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,7)) # Set figure size\n",
    "ax = sns.barplot(x = x, y = active_dups, palette = 'Greens', edgecolor = 'b')\n",
    "ax.set_title(\"Active Status Duplicates by Identifier\", fontsize=18)\n",
    "ax.set_xlabel(\"Identifier\", fontsize=18)\n",
    "ax.set_xticklabels(x, rotation=90, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot comparing duplicates considered Not Active\n",
    "inactive_dups =[]\n",
    "for dup in dup_list:\n",
    "    inactive_dups.append(len(dup[dup['STATUS']!='ACTIVE']))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,7)) # Set figure size\n",
    "ax = sns.barplot(x = x, y = inactive_dups, palette = 'Reds', edgecolor = 'b')\n",
    "ax.set_title(\"Not Active Status Duplicates by Identifier\", fontsize=18)\n",
    "ax.set_xlabel(\"Identifier\", fontsize=18)\n",
    "ax.set_xticklabels(x, rotation=90, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outer join Clean Whole Names, Paso 5 ID, Soundex, Metaphone\n",
    "dup1_results = dup1[['ID','FIRST_NAME','MIDDLE_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','STATUS','raw_wholename_id']]\n",
    "dup2_results = dup2[['ID','FIRST_NAME','MIDDLE_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','STATUS','clean_wholename_id']]\n",
    "dup8_results = dup8[['ID','FIRST_NAME','MIDDLE_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','STATUS','partialnames_id']]\n",
    "dup14_results = dup14[['ID','FIRST_NAME','MIDDLE_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','STATUS','soundex_id']]\n",
    "dup20_results = dup20[['ID','FIRST_NAME','MIDDLE_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','STATUS','metaphone_id']]\n",
    "\n",
    "# mobile number results\n",
    "dup3_results = dup3[['ID','FIRST_NAME','MIDDLE_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','STATUS','clean_wholename_mobile_id']]\n",
    "dup9_results = dup9[['ID','FIRST_NAME','MIDDLE_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','STATUS','partialnames_mobile_id']]\n",
    "dup15_results = dup15[['ID','FIRST_NAME','MIDDLE_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','STATUS','soundex_mobile_id']]\n",
    "dup21_results = dup21[['ID','FIRST_NAME','MIDDLE_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','STATUS','metaphone_mobile_id']]\n",
    "\n",
    "results_list = [dup1_results,dup2_results, dup8_results, dup14_results, dup20_results]\n",
    "\n",
    "result1 = reduce(lambda left,right: pd.merge(left,right,on=['ID','FIRST_NAME','MIDDLE_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','STATUS'],how='outer'), results_list)\n",
    "print(len(result1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [len(dup1_results),len(dup2_results),len(dup20_results),len(dup14_results),len(dup8_results)]\n",
    "counts_pct = [len(dup1_results)/len(d),len(dup2_results)/len(d),\n",
    "              len(dup20_results)/len(d),len(dup14_results)/len(d),len(dup8_results)/len(d)]\n",
    "counts_pct=np.multiply(counts_pct,100)\n",
    "count_labels = ['Raw Whole Names + Full Birthdate','Clean Whole Names + Full Birthdate',\n",
    "                'Metaphone + Full Birthdate','Soundex + Full Birthdate','Partial Names (Paso 5) + Full Birthdate']\n",
    "dup_df = pd.DataFrame(list(zip(count_labels,counts,counts_pct)),columns =['Labels','Counts','Counts Percentage'])\n",
    "#index=list(reversed(range(8)))\n",
    "dup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot counts\n",
    "ax = dup_df.pivot(columns='Labels',values='Counts').plot(kind='barh',stacked=True,\n",
    "                                                           colormap='Blues',rot=90,figsize=(15,7)) \n",
    "ax.set_title(\"Duplicates Identified by Biographic Methods in SCOPE for all Colombia\", fontsize=16)\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1.05, 0.9), ncol=1)\n",
    "ax.set_xlabel(\"Counts\", fontsize=14)\n",
    "ax.set_ylabel(\"Method\", fontsize=14)\n",
    "ax.set_yticklabels(dup_df.Labels, rotation=0, fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot percentage\n",
    "ax = dup_df.pivot(columns='Labels',values='Counts Percentage').plot(kind='barh',stacked=True,\n",
    "                                                           colormap='Reds',rot=90,figsize=(15,7)) \n",
    "ax.set_title(\"Duplicates Identified by Various Methods (in Percent of Total)\", fontsize=16)\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1.05, 0.9), ncol=1)\n",
    "ax.set_xlabel(\"Percent (%)\", fontsize=14)\n",
    "ax.set_ylabel(\"Method\", fontsize=14)\n",
    "ax.set_yticklabels(dup_df.Labels, rotation=0, fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "catvardistribution(result1, 'STATUS', x=10,y=5,rot=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outer join Clean Whole Names-Decomposed, Partial letters, Soundex-Decomposed, Metaphone-Decomposed\n",
    "dup7_results = dup7[['ID','FIRST_NAME','MIDDLE_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','STATUS','clean_wholename_decomposed_id']]\n",
    "dup13_results = dup13[['ID','FIRST_NAME','MIDDLE_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','STATUS','partialnames_decomposed_id']]\n",
    "dup19_results = dup19[['ID','FIRST_NAME','MIDDLE_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','STATUS','soundex_decomposed_id']]\n",
    "dup25_results = dup25[['ID','FIRST_NAME','MIDDLE_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','STATUS','metaphone_decomposed_id']]\n",
    "\n",
    "results_list = [dup7_results, dup13_results, dup19_results, dup25_results]\n",
    "\n",
    "result2 = reduce(lambda left,right: pd.merge(left,right,on=['ID','FIRST_NAME','MIDDLE_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','STATUS'],how='outer'), results_list)\n",
    "print(len(result2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "catvardistribution(result2, 'STATUS', x=10,y=5,rot=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare clean whole names, soundex and metaphone identifiers\n",
    "results_list = [dup2_results, dup3_results, dup14_results, dup15_results, dup20_results, dup21_results]\n",
    "\n",
    "result3 = reduce(lambda left,right: pd.merge(left,right,on=['ID','FIRST_NAME','MIDDLE_NAME','LAST_NAME','GENDER','DATE_OF_BIRTH','STATUS'],how='outer'), results_list)\n",
    "print(len(result3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#high confidence duplicates\n",
    "# inner join original data, and list of duplicates applying Clean Whole Names, Soundex and Metaphone methods\n",
    "result3_copy = result3[['ID','clean_wholename_id','clean_wholename_mobile_id','soundex_id','soundex_mobile_id','metaphone_id','metaphone_mobile_id']]\n",
    "results_list = [d_original,result3_copy]\n",
    "\n",
    "result_highconfidence_scope = reduce(lambda left,right: pd.merge(left,right,on=['ID'],how='inner'), results_list)\n",
    "result_highconfidence_scope = result_highconfidence_scope.sort_values(by=['soundex_id'])\n",
    "\n",
    "# drop complete row duplicates\n",
    "result_highconfidence_scope = result_highconfidence_scope.drop_duplicates(keep='first')\n",
    "print(len(result_highconfidence_scope))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATUS distribution of high confidence duplicates\n",
    "catvardistribution(result_highconfidence_scope, 'STATUS',x=7,y=5,rot=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of high confidence duplicates across regions\n",
    "catvardistribution(data=result_highconfidence_scope,var='REGION',x=15,y=7,rot=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns - retain REGION column\n",
    "dup4_copy = dup4.drop(list(dup4.columns[75:92]),axis=1)\n",
    "dup4_copy = dup4_copy.drop(list(dup4_copy.columns[76:]),axis=1)\n",
    "dup6_copy = dup6.drop(list(dup6.columns[75:92]),axis=1)\n",
    "dup6_copy = dup6_copy.drop(list(dup6_copy.columns[76:]),axis=1)\n",
    "dup8_copy = dup8.drop(list(dup8.columns[75:92]),axis=1)\n",
    "dup8_copy = dup8_copy.drop(list(dup8_copy.columns[76:]),axis=1)\n",
    "result_highconfidence_scope_copy = result_highconfidence_scope.drop(list(result_highconfidence_scope.columns[75:92]),axis=1)\n",
    "result_highconfidence_scope_copy = result_highconfidence_scope_copy.drop(list(result_highconfidence_scope_copy.columns[76:]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for review of potential false positives\n",
    "dup4_copy.to_excel(f\"{directory_output}/partialnames_duplicates.xlsx\", index=False)\n",
    "dup6_copy.to_excel(f\"{directory_output}/soundex_duplicates.xlsx\", index=False)\n",
    "dup8_copy.to_excel(f\"{directory_output}/metaphone_duplicates.xlsx\", index=False)\n",
    "result_highconfidence_scope_copy.to_excel(f\"{directory_output}/duplicates_high_confidence_{date}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove high confidence duplicates from original dataset\n",
    "duplicates_to_remove = result_highconfidence_scope\n",
    "\n",
    "# set ID as the index\n",
    "duplicates_to_remove = duplicates_to_remove.set_index('ID')\n",
    "d_original = d_original.set_index('ID')\n",
    "\n",
    "# data with partial names and high confidence duplicates from Arauca removed \n",
    "d_deduplicado = d_original.drop(duplicates_to_remove.index)\n",
    "\n",
    "# reset index\n",
    "#dup4_copy = dup4_copy.reset_index()\n",
    "d_original = d_original.reset_index()\n",
    "result_highconfidence_scope_copy = result_highconfidence_scope_copy.reset_index()\n",
    "d_deduplicado = d_deduplicado.reset_index()\n",
    "\n",
    "# IDs from exclude list removed\n",
    "print(f'Total size of dataset after deduplication: {len(d_deduplicado)}')\n",
    "print(len(d_original)-len(d_deduplicado))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only heads of household\n",
    "d_deduplicado_onlyheads = d_deduplicado[d_deduplicado.HOUSEHOLD_ROLE=='HEAD']\n",
    "len(d_deduplicado_onlyheads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the deduplicated dataset\n",
    "d_deduplicado.to_excel(f\"{directory_output}/no_duplicados_{date}.xlsx\", index=False)\n",
    "d_deduplicado_onlyheads.to_excel(f\"{directory_output}/no_duplicados_solo_cabezas_{date}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
